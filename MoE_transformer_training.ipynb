{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aralmeida\\Anaconda3\\envs\\alien_test\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] No se encontrÃ³ el proceso especificado'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "from MoE_based_transformer import MoE_ViT_classifier\n",
    "\n",
    "\n",
    "# If available, CUDA (in this case we use CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "\n",
    "\n",
    "# Model parameters (same as baseline for fairest possible comparison)\n",
    "size = 32\n",
    "bs = 512 # Batch size\n",
    "lr = 0.0001\n",
    "epochs = 5 # 200 would be nice but it takes too long\n",
    "patch = 4\n",
    "use_amp = False\n",
    "\n",
    "# Applying some transformation to both sets \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "# Prepare CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8)\n",
    "\n",
    "# 10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_patches 64\n",
      "patch_dim 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aralmeida\\AppData\\Local\\Temp\\ipykernel_2240\\3761003703.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Sat Aug  3 01:42:22 2024 Epoch 0, lr: 0.0001000, val loss: 153.97998, acc: 44.27000\n",
      "[153.97997963428497]\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aralmeida\\Anaconda3\\envs\\alien_test\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug  3 03:31:53 2024 Epoch 1, lr: 0.0000905, val loss: 134.96651, acc: 51.49000\n",
      "[153.97997963428497, 134.96650767326355]\n",
      "\n",
      "Epoch: 2\n",
      "Sat Aug  3 05:22:03 2024 Epoch 2, lr: 0.0001000, val loss: 128.28579, acc: 54.36000\n",
      "[153.97997963428497, 134.96650767326355, 128.2857929468155]\n",
      "\n",
      "Epoch: 3\n",
      "Sat Aug  3 07:11:27 2024 Epoch 3, lr: 0.0000905, val loss: 119.47903, acc: 57.75000\n",
      "[153.97997963428497, 134.96650767326355, 128.2857929468155, 119.47903341054916]\n",
      "\n",
      "Epoch: 4\n",
      "Sat Aug  3 08:28:45 2024 Epoch 4, lr: 0.0000655, val loss: 108.42445, acc: 61.19000\n",
      "[153.97997963428497, 134.96650767326355, 128.2857929468155, 119.47903341054916, 108.42444515228271]\n"
     ]
    }
   ],
   "source": [
    "# MoE-based ViT call\n",
    "net = MoE_ViT_classifier(image_size = 32, \n",
    "                        patch_size = 4,\n",
    "                        num_classes = 10,\n",
    "                        dim=412, \n",
    "                        depth = 6,  \n",
    "                        heads = 8, \n",
    "                        moe_input_size =  10, # MoE parameters\n",
    "                        moe_output_dim = 412, \n",
    "                        moe_num_experts = 8, \n",
    "                        moe_hidden_dim = 512, # same as baseline MLP dim \n",
    "                        moe_noisy_gating = False,\n",
    "                        moe_k = 4, \n",
    "                        num_experts_per_tok = 2, \n",
    "                        channels = 3, \n",
    "                        dim_head=412, \n",
    "                        emb_dropout=0,)\n",
    "\n",
    "# Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# use cosine scheduling\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "# Train and test functions \n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return train_loss/(batch_idx+1)\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    # Accuraccy \n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "    \n",
    "    # Save in a txt accurary and loss\n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, val loss: {test_loss:.5f}, acc: {(acc):.5f}'\n",
    "    print(content)\n",
    "\n",
    "    # Create txt file and append content \n",
    "    with open('MoE-based_ViT.txt', 'w') as f:\n",
    "        f.write(content)\n",
    "        f.close()\n",
    "\n",
    "    return test_loss, acc\n",
    "\n",
    "list_loss = []\n",
    "list_acc = []\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    start = time.time()\n",
    "    trainloss = train(epoch)\n",
    "    val_loss, acc = test(epoch)\n",
    "    \n",
    "    scheduler.step(epoch-1) # step cosine scheduling\n",
    "    \n",
    "    list_loss.append(val_loss)\n",
    "    list_acc.append(acc)\n",
    "\n",
    "\n",
    "    print(list_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alien_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
